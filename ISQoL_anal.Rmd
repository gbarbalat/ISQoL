---
title: "Impact of Insight on quality of life - analysis (Step 2G onwards)"
author: "Guillaume Barbalat"
date: "1/6/2022"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    code_download: true
---


# load packages
```{r warning=FALSE, message=FALSE, eval=TRUE, echo=FALSE, results=FALSE}

# Prelude- Clean data and Load packages
#first run Decipher_PEC.R script
close.screen(all=TRUE)
rm(list=ls())
header=1;

library(SuperLearner)
library(tmle)
library(earth)
library(glmnet)
library(xgboost)
library(dplyr)
```

# load data, remove missing data
```{r echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
load(file="mat_A_W_ISQoL.RData") 

#Not Dummy for table1
mat_A_W_Table1 <- mat_A_W %>% 
  dplyr::select(c(Study.Subject.ID,A,
    Age,Sex,Education,CENTRE,#Employment,Adresseur,
    RQTH,Fam,Marginalisation,Forensic,
    Dx,Dx2,Dx_SOMA,Addictions,
    GAF,CGI,
    First_Contact,N_Admissions,
    
    SQoL18_SEL:SQoL18_TOT,
    WEMWBS_TOT,
    ISMI_TOT,
    IS_TOT,
    SERS_TOT,
    STORI)) %>%
  filter(Dx=="SCZ" & !is.na(A))%>%
  as.data.frame(stringsAsFactors = TRUE) 



#alldum with NA
#SL_data_MHC <- mat_A_W_rd_PEC_6dx_MHC_alldum_NA

#dum without linear, completed cases
#SL_data_cc_MHC <- mat_A_W_rd_PEC_6dx_MHC_alldum_cc[complete.cases(mat_A_W_rd_PEC_6dx_MHC_alldum_cc),]

#dum with linear, completed cases
#SL_data_cc_MHC_linear <- mat_A_W_rd_PEC_6dx_MHC_linear_cc[complete.cases(mat_A_W_rd_PEC_6dx_MHC_linear_cc),]
```

# Missing data
```{r eval=FALSE, echo=FALSE, warning=FALSE, message=FALSE}

#Percentage of missing data per observation
missing_per_row = data.frame(StudySubjectID=mat_A_W_Table1$Study.Subject.ID,
                             pct_miss=rowSums(is.na(mat_A_W_Table1))/length(mat_A_W_Table1))

hist(missing_per_row$pct_miss)

#select everything(ends_with("_NA"))
NA_dum <- mat_A_W_alldum_NA %>%
  filter(Dx_SCZ==1 & IS_TOT_NA==0) %>%
  select(ends_with("_NA"))

#function to find the mode
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
}

#replace NA by median and mode, then cbind NA_dum (sl3 package strategy)
tmp<-mat_A_W_Table1 %>%

  group_by() %>%
  mutate(across(where(is.numeric),
                ~case_when(!is.na(.x)~as.numeric(.x),
                           is.na(.x)~as.numeric(median(.,na.rm=TRUE)))
                ),
         across(where(is.factor),
                ~case_when(!is.na(.x)~as.factor(.x),
                           is.na(.x)~factor(find_mode(.))
                           )
                )
  ) %>%
  cbind(NA_dum)
  
# take out subj who miss a lot of data
# who_miss_50=filter(missing_per_row,round(pct_miss,1)>=0.5)
# who_miss_40=filter(missing_per_row,round(pct_miss,1)>=0.4)
# who_miss_30=filter(missing_per_row,round(pct_miss,1)>=0.3)
# who_miss_20=filter(missing_per_row,round(pct_miss,1)>=0.2)
# who_miss_10=filter(missing_per_row,round(pct_miss,1)>=0.1)
# 
# SL_data_50_MHC<-anti_join(SL_data_MHC,who_miss_50,by="StudySubjectID") 
# SL_data_40_MHC<-anti_join(SL_data_MHC,who_miss_40,by="StudySubjectID") 
# SL_data_30_MHC<-anti_join(SL_data_MHC,who_miss_30,by="StudySubjectID") 
# SL_data_20_MHC<-anti_join(SL_data_MHC,who_miss_20,by="StudySubjectID") 
# SL_data_10_MHC<-anti_join(SL_data_MHC,who_miss_10,by="StudySubjectID") 
# 
```

# Step 2G- Univariate and Bivariate analysis aka Table 1
```{r warning=FALSE, message=FALSE, echo=FALSE}
#Table1_data <- mat_A_W_Table1[complete.cases(mat_A_W_Table1),]
Table1_data <- dplyr::select(mat_A_W_Table1,-Study.Subject.ID)
  
###############
library(table1)
my.render.cont <- function(x) {
    with(stats.default(x), 
         sprintf("%0.2f (%0.1f)", MEAN, SD))
}


pvalue <- function(x, ...) {
    # Construct vectors of data y, and groups (strata) g
    y <- unlist(x)
    g <- factor(rep(1:length(x), times=sapply(x, length)))
    if (is.numeric(y)) {
        # For numeric variables, perform a standard 2-sample t-test
        p <- t.test(y ~ g)$p.value
    } else {
        # For categorical variables, perform a chi-squared test of independence
        p <- chisq.test(table(y, g))$p.value
    }
    # Format the p-value, using an HTML entity for the less-than sign.
    # The initial empty string places the output on the line below the variable label.
    c("", sub("<", "&lt;", format.pval(p, digits=3, eps=0.001)))
}

table1(~ .
       | A,
       data=Table1_data,  
       rowlabelhead = "Variables",
       overall=F, extra.col=list(`P-value`=pvalue)
       #,render.continuous=my.render.cont
       )

###############
```


# Steps 3- Specify the counterfactuals and the causal parameter of interest (e.g. ATE, MSM, MTP ...)

$$\theta^*_{ATE}=E[Y_{A=1}]-E[Y_{A=0}]$$
$$E[Y]=m(\beta|A,V)=\beta_0+\beta_1A+\beta_2V+\beta_3A*V$$


# Step 4- Identify (backdoor path and positivity - gross analysis, otherwise see Step 6A)

strong strong temporality issues  
checking positivity assumptions post-hoc  


# Step 5-  Statistical strategy (e.g. G methods, best fit using SL etc ... Which R packages and functions)  

causal-ish question ... G methods using tmle (doubly-robust estimators).  
`tmle` package for an ATE  
`causalglm` and other `tmle3` functions  

# Step 6A- Estimate(main arguments)

Complete.Cases vs. strategy for missing values  
Raw measures vs. dummyfications  
V=10 N btw 500 and 1000  
binary A (IS cutoff = 9)  
continuous Y???  
SL algorithms: LASSO, MARS, step, stepIA, mean (benchmark), randomForest, xgboost, ...  
Use `caret` and `tmle3`  

## Other algorithms
```{r echo=FALSE, eval=FALSE}
listWrappers()

SL.hal9001<- function (Y, X, newX = NULL, family = "gaussian",
                       obsWeights = rep(1, length(Y)), id = NULL, max_degree = ifelse(ncol(X) >= 20,2, 3),
                       smoothness_orders = 1, num_knots = ifelse(smoothness_orders >= 1, 25, 50),
                       reduce_basis = 1/sqrt(length(Y)), lambda = NULL,
                       ...)
{
  if (!is.matrix(X))
    X <- as.matrix(X)
  if (!is.null(newX) & !is.matrix(newX))
    newX <- as.matrix(newX)
  hal_fit <- fit_hal(X = X, Y = Y, family = family$family,
                     fit_control = list(weights = obsWeights), id = id, max_degree = max_degree,
                     smoothness_orders = smoothness_orders, num_knots = num_knots,
                     reduce_basis = reduce_basis, lambda = lambda)
  if (!is.null(newX)) {
    pred <- stats::predict(hal_fit, new_data = newX)
  }
  else {
    pred <- stats::predict(hal_fit, new_data = X)
  }
  fit <- list(object = hal_fit)
  #class(fit) <- "SL.hal9001"
  out <- list(pred = pred, fit = fit)
  return(out)
}

SL.step_IA_glmnet <- function (Y, X, newX, family="gaussian", direction = "backward", trace = 0,
                        k = 2, ...)
{
   whichVarialbes <- screen.glmnet(Y,X, family="gaussian")#returns logical vector
   
  fit.glm <- glm(Y ~ ., data = X[,whichVarialbes], family = "gaussian")
  fit.stepMain <- step(fit.glm, direction = direction)

  fit.step <- step(fit.stepMain, scope = Y ~ .^2, direction = direction,
                   trace = trace, k = k)
  pred <- predict(fit.step, newdata = newX, type = "response")
  fit <- list(object = fit.step)
  out <- list(pred = pred, fit = fit)
  class(out$fit) <- c("SL.step")
  return(out)
}
```

## ATE
``` {r}
SL.library.A=c("SL.glm","SL.glmnet","SL.earth","SL.ranger")
SL.library.Y=list("SL.glm","SL.glmnet","SL.ranger","SL.earth")#,"SL.hal9001","SL.step_IA_glmnet")

mat_Y_A_W=select(mat_A_W_Table1,-c(CENTRE,Dx,IS_TOT))
mat_Y_A_W=mat_Y_A_W[complete.cases(mat_Y_A_W),]

A=as.numeric(mat_Y_A_W$A)
W=select(mat_Y_A_W,-c(starts_with("SQoL18"),A,
                      WEMWBS_TOT,ISMI_TOT,SERS_TOT,STORI))
W<-W %>%
  mutate(across(everything(.),~as.numeric(.)))

#summary(lm(SQoL18_TOT~., data=mat_Y_A_W))
run_tmle <- function(Y) {

result <- tmle(Y,A,W,
     Q.SL.library=SL.library.Y ,
     g.SL.library=SL.library.A ,
     V=5,
     family="gaussian")
return(result)
}
(results_TOT <- run_tmle(Y=mat_Y_A_W$SQoL18_TOT))
(AUT <- run_tmle(Y=mat_Y_A_W$SQoL18_AUT))
(RES <- run_tmle(Y=mat_Y_A_W$SQoL18_RES))
(FAM <- run_tmle(Y=mat_Y_A_W$SQoL18_FAM))
(FRI <- run_tmle(Y=mat_Y_A_W$SQoL18_FRI))
(SEL <- run_tmle(Y=mat_Y_A_W$SQoL18_SEL))
(PSY <- run_tmle(Y=mat_Y_A_W$SQoL18_PSY))
(PHY <- run_tmle(Y=mat_Y_A_W$SQoL18_PHY))
(ROM <- run_tmle(Y=mat_Y_A_W$SQoL18_ROM))


```

## Positivity
```{r eval=FALSE}

summary(results_TOT$g$g1W)
summary(sum(results_TOT$g$g1W)/results_TOT$g$g1W)

hist(1/results_TOT$g$g1W)
hist(
  (1/results_TOT$g$g1W) /
      sum( (1/results_TOT$g$g1W))
  )


```


## MSM

```{r}
SL.library.A=c("SL.glm","SL.glmnet","SL.earth","SL.ranger")
SL.library.Y=list("SL.glm","SL.glmnet","SL.ranger","SL.earth")#,"SL.hal9001","SL.step_IA_glmnet")

mat_Y_A_W=select(mat_A_W_Table1,-c(CENTRE,Dx,IS_TOT))
mat_Y_A_W=mat_Y_A_W[complete.cases(mat_Y_A_W),]
#potential_moderators=c("WEMWBS_TOT","ISMI_TOT","SERS_TOT","STORI")
W=select(mat_Y_A_W,-c(starts_with("SQoL18"),A,Study.Subject.ID))
W=W %>%
  mutate(across(everything(.),~as.numeric(.)))
which_Z=c("WEMWBS_TOT","ISMI_TOT","SERS_TOT","STORI")
potential_moderators=W %>%
  select(-"STORI") %>%
  colnames()

A=as.numeric(mat_Y_A_W$A)

moder_anal<-function(which_V, W) {
  
 
which_Z=c("WEMWBS_TOT","ISMI_TOT","SERS_TOT","STORI")
which_Z=which_Z[!which_Z %in% which_V]# except if considered moderators

W<-W %>%
  select(-which_Z) %>%
  mutate(across(everything(.),~as.numeric(.)))

V<-W %>%
  select(all_of(which_V)) %>%
  rename(which_V=all_of(which_V)) #%>%
  # mutate(across(starts_with("SQoL18"),~cut(.x, breaks=2,include.lowest=TRUE, ordered_result = TRUE, labels=c(1,0)))) %>%
  # mutate(across(starts_with("SQoL18"),~as.numeric(.)))
  #mutate(which_V=cut(which_V, breaks=2,include.lowest=TRUE, ordered_result = TRUE, labels=c(0,1))) 
V <- as.numeric(as.matrix(V))

Y=mat_Y_A_W$SQoL18_TOT

results_MSM <- tmleMSM(Y, A, W, V, 
        MSM="A*V",
        T = rep(1,length(Y)), Delta = rep(1, length(Y)), 
        v = NULL, Q = NULL, Qform = NULL, Qbounds = c(-Inf, Inf), 
        Q.SL.library = SL.library.Y, g.SL.library = SL.library.A,
        cvQinit = TRUE, hAV = NULL, hAVform = NULL, g1W = NULL, 
        gform = NULL, pDelta1 = NULL, g.Deltaform = NULL, 
        ub = 1/0.025, family = "gaussian", fluctuation = "logistic", 
        alpha  = 0.995, id = 1:length(Y), V_SL = 5, inference = TRUE, 
        verbose = TRUE, Q.discreteSL = FALSE, g.discreteSL = FALSE) 
}


for (i in (1:length(potential_moderators))) {
  print(potential_moderators[i])
  print(results_MSM <- moder_anal(which_V=potential_moderators[i],W))

}
#(results_MSM <- moder_anal("STORI_TOT")) need numeric V?

#positivity assumption
#hist(1/results_MSM$g$g1W)
#hist(results_MSM$g.AV$g1W/results_MSM$g$g1W)


```


# Step 6B- Sensitivity analysis (wt, Y, subset, A, NA, IA)


# Network analysis

```{r echo=FALSE, eval=FALSE}
library(qgraph)
library(huge)
library(bootnet)
library(NetworkComparisonTest)

#Rough network
SEL_IS1=mat_Y_A_W$SQoL18_SEL;ROM_IS1=mat_Y_A_W$SQoL18_ROM;PHY_IS1=mat_Y_A_W$SQoL18_PHY;
PSY_IS1=mat_Y_A_W$SQoL18_PSY;RES_IS1=mat_Y_A_W$SQoL18_RES;AUT_IS1=mat_Y_A_W$SQoL18_AUT;
FRI_IS1=mat_Y_A_W$SQoL18_FRI;FAM_IS1=mat_Y_A_W$SQoL18_FAM;
Network_df_IS1=data.frame(SEL_IS1,RES_IS1,AUT_IS1,PHY_IS1,FAM_IS1,FRI_IS1,ROM_IS1,PSY_IS1)
CorMat_IS1=cor_auto(Network_df_IS1, npn.SKEPTIC = FALSE)
Q_IS1=qgraph(CorMat_IS1, graph = "glasso", sampleSize = nrow(Network_df_IS1),
         tuning =0.5, layout = "circle", title = "BIC", details = TRUE,
         threshold=TRUE)
C_IS1=centralityPlot(Q_IS1);

#comparing network function
network_first_anal <- function(Network_df_IS1,Network_df_IS0,npn.SKEPTIC) {
  ##########
  #CorMat=cor_auto(select(Network_df,!id))
  ##########
  CorMat_IS1=cor_auto(Network_df_IS1, npn.SKEPTIC=npn.SKEPTIC)
  CorMat_IS0=cor_auto(Network_df_IS0, npn.SKEPTIC=npn.SKEPTIC)

  
  #The glasso is run for 100 values of the tuning parameter logarithmically spaced between the maximal value of 
  #the tuning parameter at which all edges are zero, lamba_max, and lambda_max/100. 
  #For each of these graphs the EBIC is computed and the graph with the best EBIC is selected.
  #The partial correlation matrix is computed using wi2net and returned.
  
  #BICgraph <- EBICglasso(S=CorMat, n=nrow(Network_df), gamma=0, threshold = TRUE)#BIC
  Q_IS1=qgraph(CorMat_IS1, graph = "glasso", sampleSize = nrow(Network_df_IS1),
         tuning =0.5, layout = "circle", title = "BIC", details = TRUE,
         threshold=TRUE)
  C_IS1=centralityPlot(Q_IS1);#centralityTable(Q)
  
  Q_IS0=qgraph(CorMat_IS0, graph = "glasso", sampleSize = nrow(Network_df_IS0),
         tuning =0.5, layout = "circle", title = "BIC", details = TRUE, threshold=TRUE)
  C_IS0=centralityPlot(Q_IS0);#centralityTable(Q)
  
  
  #comparing networks
  colnames(Network_df_IS1)=c("SEL_IS0", "RES_IS0", "AUT_IS0", "PHY_IS0", "FAM_IS0", "FRI_IS0", "ROM_IS0", "PSY_IS0")
    
    ##Network comparison test
    testing_NCT <- NCT(Network_df_IS1,Network_df_IS0,
                       gamma = 0.5, it= 1000, binary.data=FALSE, 
      paired=FALSE, weighted=TRUE, AND=TRUE, abs=TRUE,
      test.edges=FALSE, edges="all", 
      progressbar=TRUE, make.positive.definite=TRUE,
      p.adjust.methods="fdr", #c("none","holm","hochberg","hommel",
                        #  "bonferroni","BH","BY","fdr"), 
      test.centrality=TRUE, 
      centrality=c("strength","expectedInfluence"),nodes="all",
      communities=NULL,useCommunities="all",
      #estimator, estimatorArgs = list(), 
      verbose = TRUE
      )
    
  
  return(list(Q_IS1,C_IS1,Q_IS0,C_IS0,testing_NCT))
}

#Rough network IS1 vs. IS0
SEL_IS1=mat_Y_A_W$SQoL18_SEL[mat_Y_A_W$A==1];
ROM_IS1=mat_Y_A_W$SQoL18_ROM[mat_Y_A_W$A==1];
PHY_IS1=mat_Y_A_W$SQoL18_PHY[mat_Y_A_W$A==1];
PSY_IS1=mat_Y_A_W$SQoL18_PSY[mat_Y_A_W$A==1];
RES_IS1=mat_Y_A_W$SQoL18_RES[mat_Y_A_W$A==1];
AUT_IS1=mat_Y_A_W$SQoL18_AUT[mat_Y_A_W$A==1];
FRI_IS1=mat_Y_A_W$SQoL18_FRI[mat_Y_A_W$A==1];
FAM_IS1=mat_Y_A_W$SQoL18_FAM[mat_Y_A_W$A==1];
Network_df_IS1=data.frame(SEL_IS1,RES_IS1,AUT_IS1,PHY_IS1,FAM_IS1,FRI_IS1,ROM_IS1,PSY_IS1)

SEL_IS0=mat_Y_A_W$SQoL18_SEL[mat_Y_A_W$A==0];
ROM_IS0=mat_Y_A_W$SQoL18_ROM[mat_Y_A_W$A==0];
PHY_IS0=mat_Y_A_W$SQoL18_PHY[mat_Y_A_W$A==0];
PSY_IS0=mat_Y_A_W$SQoL18_PSY[mat_Y_A_W$A==0];
RES_IS0=mat_Y_A_W$SQoL18_RES[mat_Y_A_W$A==0];
AUT_IS0=mat_Y_A_W$SQoL18_AUT[mat_Y_A_W$A==0];
FRI_IS0=mat_Y_A_W$SQoL18_FRI[mat_Y_A_W$A==0];
FAM_IS0=mat_Y_A_W$SQoL18_FAM[mat_Y_A_W$A==0];
Network_df_IS0=data.frame(SEL_IS0,RES_IS0,AUT_IS0,PHY_IS0,FAM_IS0,FRI_IS0,ROM_IS0,PSY_IS0)

rough <- network_first_anal(Network_df_IS1 = Network_df_IS1,Network_df_IS0 = Network_df_IS0, npn.SKEPTIC = FALSE)


#IPW transf
SEL_IS1=mat_Y_A_W$SQoL18_SEL[mat_Y_A_W$A==1]*SEL$g$g1W[mat_Y_A_W$A==1];
ROM_IS1=mat_Y_A_W$SQoL18_ROM[mat_Y_A_W$A==1]*ROM$g$g1W[mat_Y_A_W$A==1];
PHY_IS1=mat_Y_A_W$SQoL18_PHY[mat_Y_A_W$A==1]*PHY$g$g1W[mat_Y_A_W$A==1];
PSY_IS1=mat_Y_A_W$SQoL18_PSY[mat_Y_A_W$A==1]*PSY$g$g1W[mat_Y_A_W$A==1];
RES_IS1=mat_Y_A_W$SQoL18_RES[mat_Y_A_W$A==1]*RES$g$g1W[mat_Y_A_W$A==1];
AUT_IS1=mat_Y_A_W$SQoL18_AUT[mat_Y_A_W$A==1]*AUT$g$g1W[mat_Y_A_W$A==1];
FRI_IS1=mat_Y_A_W$SQoL18_FRI[mat_Y_A_W$A==1]*FRI$g$g1W[mat_Y_A_W$A==1];
FAM_IS1=mat_Y_A_W$SQoL18_FAM[mat_Y_A_W$A==1]*FAM$g$g1W[mat_Y_A_W$A==1];
Network_df_IS1=data.frame(SEL_IS1,RES_IS1,AUT_IS1,PHY_IS1,FAM_IS1,FRI_IS1,ROM_IS1,PSY_IS1)

SEL_IS0=mat_Y_A_W$SQoL18_SEL[mat_Y_A_W$A==0]*SEL$g$g1W[mat_Y_A_W$A==0];
ROM_IS0=mat_Y_A_W$SQoL18_ROM[mat_Y_A_W$A==0]*ROM$g$g1W[mat_Y_A_W$A==0];
PHY_IS0=mat_Y_A_W$SQoL18_PHY[mat_Y_A_W$A==0]*PHY$g$g1W[mat_Y_A_W$A==0];
PSY_IS0=mat_Y_A_W$SQoL18_PSY[mat_Y_A_W$A==0]*PSY$g$g1W[mat_Y_A_W$A==0];
RES_IS0=mat_Y_A_W$SQoL18_RES[mat_Y_A_W$A==0]*RES$g$g1W[mat_Y_A_W$A==0];
AUT_IS0=mat_Y_A_W$SQoL18_AUT[mat_Y_A_W$A==0]*AUT$g$g1W[mat_Y_A_W$A==0];
FRI_IS0=mat_Y_A_W$SQoL18_FRI[mat_Y_A_W$A==0]*FRI$g$g1W[mat_Y_A_W$A==0];
FAM_IS0=mat_Y_A_W$SQoL18_FAM[mat_Y_A_W$A==0]*FAM$g$g1W[mat_Y_A_W$A==0];
Network_df_IS0=data.frame(SEL_IS0,RES_IS0,AUT_IS0,PHY_IS0,FAM_IS0,FRI_IS0,ROM_IS0,PSY_IS0)

i_p_w <- network_first_anal(Network_df_IS1 = Network_df_IS1,Network_df_IS0 = Network_df_IS0, npn.SKEPTIC = TRUE)

#Gcomp transf, not updating the initial estimation
SEL_IS1=SEL$Qinit$Q[,2];ROM_IS1=ROM$Qinit$Q[,2];PHY_IS1=PHY$Qinit$Q[,2];PSY_IS1=PSY$Qinit$Q[,2];
RES_IS1=RES$Qinit$Q[,2];AUT_IS1=AUT$Qinit$Q[,2];FRI_IS1=FRI$Qinit$Q[,2];FAM_IS1=FAM$Qinit$Q[,2];
Network_df_IS1=data.frame(SEL_IS1,RES_IS1,AUT_IS1,PHY_IS1,FAM_IS1,FRI_IS1,ROM_IS1,PSY_IS1)

SEL_IS0=SEL$Qinit$Q[,1];ROM_IS0=ROM$Qinit$Q[,1];PHY_IS0=PHY$Qinit$Q[,1];PSY_IS0=PSY$Qinit$Q[,1];
RES_IS0=RES$Qinit$Q[,1];AUT_IS0=AUT$Qinit$Q[,1];FRI_IS0=FRI$Qinit$Q[,1];FAM_IS0=FAM$Qinit$Q[,1];
Network_df_IS0=data.frame(SEL_IS0,RES_IS0,AUT_IS0,PHY_IS0,FAM_IS0,FRI_IS0,ROM_IS0,PSY_IS0)

g_comp <- network_first_anal(Network_df_IS1 = Network_df_IS1,Network_df_IS0 = Network_df_IS0, npn.SKEPTIC = TRUE)

#Ditto updating the initial estimation
SEL_IS1=SEL$Qstar[,2];ROM_IS1=ROM$Qstar[,2];PHY_IS1=PHY$Qstar[,2];PSY_IS1=PSY$Qstar[,2];
RES_IS1=RES$Qstar[,2];AUT_IS1=AUT$Qstar[,2];FRI_IS1=FRI$Qstar[,2];FAM_IS1=FAM$Qstar[,2];
Network_df_IS1=data.frame(SEL_IS1,RES_IS1,AUT_IS1,PHY_IS1,FAM_IS1,FRI_IS1,ROM_IS1,PSY_IS1)

SEL_IS0=SEL$Qstar[,1];ROM_IS0=ROM$Qstar[,1];PHY_IS0=PHY$Qstar[,1];PSY_IS0=PSY$Qstar[,1];
RES_IS0=RES$Qstar[,1];AUT_IS0=AUT$Qstar[,1];FRI_IS0=FRI$Qstar[,1];FAM_IS0=FAM$Qstar[,1];
Network_df_IS0=data.frame(SEL_IS0,RES_IS0,AUT_IS0,PHY_IS0,FAM_IS0,FRI_IS0,ROM_IS0,PSY_IS0)
  
t_m_l_e <- network_first_anal(Network_df_IS1 = Network_df_IS1,Network_df_IS0 = Network_df_IS0, npn.SKEPTIC = TRUE)



  
    Network_<-bootnet::estimateNetwork(Network_df_IS1, default = "EBICglasso", tuning=0.5)
    
    ##########
    #stability of centrality indices
    ##########
    boot2 <- bootnet(Network_, nBoots = 100, type = "case", nCores = 8, 
                   statistics = c("strength", "closeness", "betweenness"))
    print(plot(boot2, statistics = c("betweenness", "closeness", "strength"), order = "id",
               onlyNonZero = FALSE))
  
    print(corStability(boot2))
  
    # ##########
    # #CI of edge-weight
    # ##########
    # boot1<-bootnet(Network, nBoots = 1000, nCores = 8)
    # print(plot(boot1, labels = TRUE, order = "id", onlyNonZero = FALSE))#order="sample"
    
    # ##########
    # #testing for significant differences
    # ##########
    # ##edge-weight
    # EW_PLOT=plot(boot1, "edge", plot = "difference", onlyNonZero = FALSE, order = "id")
    # print(EW_PLOT)
    # EW_PLOT$data$upper_lower=ifelse(EW_PLOT$data$lower>0 & EW_PLOT$data$upper>0,1,0)
    # tmp=tapply(EW_PLOT$data$upper_lower,
    #        EW_PLOT$data$id2, 
    #        function(x) sum(x>0))
    # Quantif_EW_SUP=data.frame(edge_name=names(tmp), 
    #                         n_edge=tmp)
    # 
    # ##centrality
    # #differenceTest(boot1, 3, 8, "strength")
    # print(plot(boot1, "strength",order = "id",onlyNonZero = FALSE))

```
